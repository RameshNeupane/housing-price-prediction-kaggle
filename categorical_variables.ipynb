{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "\n",
    "### Three approaches\n",
    "1. Drop categorical variable\n",
    "2. Ordinal encoding\n",
    "3. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read the data\n",
    "X = pd.read_csv('./train.csv', index_col='Id')\n",
    "X_test = pd.read_csv('./test.csv', index_col='Id')\n",
    "\n",
    "# remove rows with missing target\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "# separate target from predictors\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# drop columns with missing values\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()]\n",
    "X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_test.drop(cols_with_missing, axis=  1, inplace=True)\n",
    "\n",
    "# split dataset into training and validation data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score_dataset function: returns MAE (Mean Absolute Error) for different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# compare different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "  model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "  model.fit(X_train, y_train)\n",
    "  preds = model.predict(X_valid)\n",
    "  return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drop Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(f\"MAE from approach 1: {score_dataset(drop_X_train, drop_X_valid, y_train, y_valid)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ordinal Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop problematic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns\n",
    "categorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "\n",
    "# columns that can be ordinally encoded\n",
    "good_categorical_cols = [col for col in categorical_cols if set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "\n",
    "# problematic columns\n",
    "bad_categorical_cols = list(set(categorical_cols) - set(good_categorical_cols))\n",
    "print(f\"Categorical columns:\\n{categorical_cols}\")\n",
    "print(f\"\\nGood categorical columns:\\n{good_categorical_cols}\")\n",
    "print(f\"\\nBad categorical columns: {bad_categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# drop bad categorical columns\n",
    "drop_bad_cat_X_train = X_train.drop(bad_categorical_cols, axis=1)\n",
    "drop_bad_cat_X_valid = X_valid.drop(bad_categorical_cols, axis=1)\n",
    "\n",
    "# apply ordinal encoder in good categorical columns\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "drop_bad_cat_X_train[good_categorical_cols] = ordinal_encoder.fit_transform(X_train[good_categorical_cols])\n",
    "drop_bad_cat_X_valid[good_categorical_cols] = ordinal_encoder.transform(X_valid[good_categorical_cols])\n",
    "\n",
    "mae_ordinal_encoding = score_dataset(drop_bad_cat_X_train, drop_bad_cat_X_valid, y_train, y_valid)\n",
    "print(f\"MAE from approach 2: {mae_ordinal_encoding}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. One-hot Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of unique entries in each categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_nunique = list(map(lambda col: X_train[col].nunique(), categorical_cols))\n",
    "dict_cat_nunique = dict(zip(categorical_cols, categorical_nunique))\n",
    "\n",
    "sorted(dict_cat_nunique.items(), key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with low cardinality\n",
    "low_cardinality_cols = [col for col in categorical_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# high cardinality columns\n",
    "high_cardinality_cols = list(set(categorical_cols) - set(low_cardinality_cols))\n",
    "\n",
    "print(f\"Columns that are one-hot encoded:\\n {low_cardinality_cols}\")\n",
    "print(f\"\\nColumns dropped due to high cardinality:\\n {high_cardinality_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "simple_imputer = SimpleImputer()\n",
    "\n",
    "# apply one-hot encoding to each low cardinality column\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "oh_cols_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "oh_cols_valid = pd.DataFrame(one_hot_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# one-hot removed index, put it back\n",
    "oh_cols_train.index = X_train.index\n",
    "oh_cols_valid.index = X_valid.index\n",
    "\n",
    "# remove categorical columns\n",
    "num_X_train = X_train.drop(categorical_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(categorical_cols, axis=1)\n",
    "\n",
    "# add one-hot encoded columns to numerical featured columns\n",
    "oh_X_train = pd.concat([num_X_train, oh_cols_train], axis=1)\n",
    "oh_X_valid = pd.concat([num_X_valid, oh_cols_valid], axis=1)\n",
    "\n",
    "# ensure all columns having string type\n",
    "oh_X_train.columns = oh_X_train.columns.astype(str)\n",
    "oh_X_valid.columns = oh_X_valid.columns.astype(str)\n",
    "# imputed_oh_X_train = pd.DataFrame(simple_imputer.fit_transform(oh_X_train))\n",
    "\n",
    "mae_oh_encoding = score_dataset(oh_X_train, oh_X_valid, y_train, y_valid)\n",
    "print(f\"MAE from approach 3 (One-hot encoding): {mae_oh_encoding}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oh_cols_test = pd.DataFrame(one_hot_encoder.transform(X_test[low_cardinality_cols]))\n",
    "oh_cols_test.index = X_test.index\n",
    "num_X_test = X_test.drop(categorical_cols, axis=1)\n",
    "oh_X_test = pd.concat([num_X_test, oh_cols_test], axis=1)\n",
    "oh_X_test.columns = oh_X_test.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of missing value in each column of training data\n",
    "missing_val_count_by_column = (oh_X_test.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_X_test.fillna(0, inplace=True)\n",
    "# define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(oh_X_train, y_train)\n",
    "preds_test = model.predict(oh_X_test)\n",
    "\n",
    "# save prediction into a file\n",
    "output = pd.DataFrame({'Id': X_test.index, 'SalePrice': preds_test})\n",
    "output.to_csv('prediction_categorical_variable.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
